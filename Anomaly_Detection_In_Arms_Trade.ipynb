{
 "cells": [
  {
   "cell_type": "code",
   "id": "4e54787a6d4f2614",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T18:37:44.586585Z",
     "start_time": "2026-01-27T18:37:44.481900Z"
    }
   },
   "source": [
    "# =========================\n",
    "# Import Libraries\n",
    "# =========================\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "import tqdm as tqdm\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.decomposition import PCA"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "c334d5531e3c05a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T18:37:30.967592Z",
     "start_time": "2026-01-27T18:37:30.947077Z"
    }
   },
   "source": [
    "# =========================\n",
    "# Helper Functions\n",
    "# =========================\n",
    "\n",
    "\n",
    "def tune_isolation_forest(\n",
    "    X: pd.DataFrame,\n",
    "    contaminations: float = np.linspace(0.01, 0.2, 10),\n",
    "    estimators: int = np.linspace(50, 200, 4),\n",
    "    n_runs: int = 10,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Tune Isolation Forest hyperparameters based on stability heuristic.\n",
    "    :param X: Training data\n",
    "    :param contaminations: Appropriate contamination levels to try\n",
    "    :param estimators: Amount of trees in the forest\n",
    "    :param n_runs: Number of runs to estimate stability\n",
    "    :return: Optimal parameters for Isolation Forest on the given data.\n",
    "    \"\"\"\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    for estimator in estimators:\n",
    "        for cont in contaminations:\n",
    "            scores = []\n",
    "            for _ in range(n_runs):\n",
    "                iso = IsolationForest(\n",
    "                    n_estimators=int(estimator),\n",
    "                    max_samples=\"auto\",\n",
    "                    contamination=cont,\n",
    "                    random_state=None,\n",
    "                )\n",
    "                scores.append(iso.fit(X).score_samples(X))\n",
    "\n",
    "            corr = []\n",
    "            for i in range(len(scores) - 1):\n",
    "                corr.append(spearmanr(scores[i], scores[i + 1])[0])\n",
    "\n",
    "            stability = np.nanmean(corr)\n",
    "\n",
    "            if stability > best_score:\n",
    "                best_score = stability\n",
    "                best_params = (int(estimator), cont)\n",
    "\n",
    "        return best_params\n",
    "\n",
    "\n",
    "def tune_lof(X: pd.DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    Tune Local Outlier Factor hyperparameters based on separation heuristic.\n",
    "    :param X: Validation data\n",
    "    :return: Optimal number of neighbors for LOF on the given data.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    candidates = [int(np.sqrt(n)), int(1.5 * np.sqrt(n)), int(2 * np.sqrt(n))]\n",
    "    candidates = [k for k in candidates if k < n]\n",
    "\n",
    "    best_k = None\n",
    "    best_sep = -np.inf\n",
    "\n",
    "    for k in candidates:\n",
    "        lof = LocalOutlierFactor(n_neighbors=k)\n",
    "        scores = -lof.fit_predict(X)\n",
    "\n",
    "        # Separation heuristic: std of LOF scores\n",
    "        sep = np.std(lof.negative_outlier_factor_)\n",
    "\n",
    "        if sep > best_sep:\n",
    "            best_sep = sep\n",
    "            best_k = k\n",
    "\n",
    "    return best_k\n",
    "\n",
    "\n",
    "def calculate_model_stability(anomaly_scores_list: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the stability of a model based on Spearman correlation.\n",
    "    :param anomaly_scores_list: List of anomaly scores from different runs of the same model\n",
    "    :return: Spearman correlation-based stability score\n",
    "    \"\"\"\n",
    "    correlation_scores = []\n",
    "    for i in range(len(anomaly_scores_list) - 1):\n",
    "        correlation, _ = spearmanr(anomaly_scores_list[i], anomaly_scores_list[i + 1])\n",
    "        correlation_scores.append(correlation)\n",
    "    stability_score = np.nanmean(correlation_scores)\n",
    "    return stability_score\n",
    "\n",
    "\n",
    "def calculate_model_agreement(model1_labels: list, model2_labels: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculate agreement between two sets of anomaly labels.\n",
    "    :param model1_labels: Labels from model 1\n",
    "    :param model2_labels: Labels from model 2\n",
    "    :return: Agreement score\n",
    "    \"\"\"\n",
    "    agreement_score = np.mean(model1_labels == model2_labels)\n",
    "    return agreement_score\n",
    "\n",
    "\n",
    "def em(\n",
    "    t_values: np.ndarray,\n",
    "    t_max_threshold: float,\n",
    "    support_volume: float,\n",
    "    uniform_scores: np.ndarray,\n",
    "    test_scores: np.ndarray,\n",
    "    num_generated_samples: int,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Compute Excess-Mass (EM) curve and its AUC.\n",
    "\n",
    "    :param t_values: Array of t values.\n",
    "    :param t_max_threshold: Maximum t threshold.\n",
    "    :param support_volume: Volume of the support.\n",
    "    :param uniform_scores: Scores of the uniform samples.\n",
    "    :param test_scores: Scores of the test samples.\n",
    "    :param num_generated_samples: Number of generated uniform samples.\n",
    "    :return: Tuple containing EM AUC, EM values, and the index of the maximum t value.\n",
    "    \"\"\"\n",
    "    em_values = np.zeros(t_values.shape[0])\n",
    "    num_test_samples = test_scores.shape[0]\n",
    "    unique_test_scores = np.unique(test_scores)\n",
    "    em_values[0] = 1.0\n",
    "\n",
    "    for score in unique_test_scores:\n",
    "        em_values = np.maximum(\n",
    "            em_values,\n",
    "            1.0 / num_test_samples * (test_scores > score).sum()\n",
    "            - t_values\n",
    "            * (uniform_scores > score).sum()\n",
    "            / num_generated_samples\n",
    "            * support_volume,\n",
    "        )\n",
    "\n",
    "    max_t_index = np.argmax(em_values <= t_max_threshold) + 1\n",
    "    if max_t_index == 1:\n",
    "        print(\"\\nFailed to achieve t_max_threshold\\n\")\n",
    "        max_t_index = -1\n",
    "\n",
    "    em_auc = auc(t_values[:max_t_index], em_values[:max_t_index])\n",
    "    return em_auc, em_values, max_t_index\n",
    "\n",
    "\n",
    "def mv(\n",
    "    alpha_values: np.ndarray,\n",
    "    support_volume: float,\n",
    "    uniform_scores: np.ndarray,\n",
    "    test_scores: np.ndarray,\n",
    "    num_generated_samples: int,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Compute Mass-Volume (MV) curve and its AUC.\n",
    "\n",
    "    :param alpha_values: Array of alpha values.\n",
    "    :param support_volume: Volume of the support.\n",
    "    :param uniform_scores: Scores of the uniform samples.\n",
    "    :param test_scores: Scores of the test samples.\n",
    "    :param num_generated_samples: Number of generated uniform samples.\n",
    "    :return: Tuple containing MV AUC and MV values.\n",
    "    \"\"\"\n",
    "    num_test_samples = test_scores.shape[0]\n",
    "    sorted_test_scores_indices = test_scores.argsort()\n",
    "    cumulative_mass = 0\n",
    "    count = 0\n",
    "    threshold_score = test_scores[sorted_test_scores_indices[-1]]\n",
    "    mv_values = np.zeros(alpha_values.shape[0])\n",
    "\n",
    "    for i in range(alpha_values.shape[0]):\n",
    "        while cumulative_mass < alpha_values[i]:\n",
    "            count += 1\n",
    "            threshold_score = test_scores[sorted_test_scores_indices[-count]]\n",
    "            cumulative_mass = 1.0 / num_test_samples * count\n",
    "        mv_values[i] = (\n",
    "            float((uniform_scores >= threshold_score).sum())\n",
    "            / num_generated_samples\n",
    "            * support_volume\n",
    "        )\n",
    "\n",
    "    mv_auc = auc(alpha_values, mv_values)\n",
    "    return mv_auc, mv_values\n",
    "\n",
    "\n",
    "def plot_country_anomalies(country_name: str):\n",
    "    \"\"\"\n",
    "    Plot time series of weighted in/out degrees and degrees for a given country,\n",
    "    and scatterplot of PCA-reduced features.\n",
    "\n",
    "    :param country_name: Name of the country to plot.\n",
    "    :return: Saves the plots to disk.\n",
    "    \"\"\"\n",
    "    country_data = (\n",
    "        feature_engineered_arms_trade_df[\n",
    "            feature_engineered_arms_trade_df[\"country\"] == country_name\n",
    "        ]\n",
    "        .sort_values(\"year\")\n",
    "        .set_index(\"year\")\n",
    "    )\n",
    "\n",
    "    # Create a full year range to expose missing years\n",
    "    full_years = np.arange(country_data.index.min(), country_data.index.max() + 1)\n",
    "    country_data_full = country_data.reindex(full_years)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(13, 8), sharex=True)\n",
    "\n",
    "    # Plot weighted volumes (TIV)\n",
    "    ax1.plot(\n",
    "        country_data_full.index,\n",
    "        country_data_full[\"weighted_out_degree\"],\n",
    "        color=\"blue\",\n",
    "        linewidth=2,\n",
    "        label=\"Exports\",\n",
    "    )\n",
    "    ax1.plot(\n",
    "        country_data_full.index,\n",
    "        country_data_full[\"weighted_in_degree\"],\n",
    "        color=\"green\",\n",
    "        linewidth=2,\n",
    "        label=\"Imports\",\n",
    "    )\n",
    "\n",
    "    # Handle missing years with dotted lines\n",
    "    for column_name, color in [\n",
    "        (\"weighted_out_degree\", \"blue\"),\n",
    "        (\"weighted_in_degree\", \"green\"),\n",
    "    ]:\n",
    "        column_values = country_data_full[column_name]\n",
    "        valid_indices = np.where(~np.isnan(column_values.values))[0]\n",
    "        for i in range(len(valid_indices) - 1):\n",
    "            start_idx, end_idx = valid_indices[i], valid_indices[i + 1]\n",
    "            if end_idx - start_idx > 1:\n",
    "                ax1.plot(\n",
    "                    [column_values.index[start_idx], column_values.index[end_idx]],\n",
    "                    [column_values.iloc[start_idx], column_values.iloc[end_idx]],\n",
    "                    color=color,\n",
    "                    linestyle=\":\",\n",
    "                    linewidth=2,\n",
    "                    alpha=0.8,\n",
    "                )\n",
    "\n",
    "    # Highlight anomalies\n",
    "    iso_anomaly_years = country_data_full.index[\n",
    "        country_data_full[\"iso_anomaly\"] & ~country_data_full[\"lof_anomaly\"]\n",
    "    ]\n",
    "    lof_anomaly_years = country_data_full.index[\n",
    "        country_data_full[\"lof_anomaly\"] & ~country_data_full[\"iso_anomaly\"]\n",
    "    ]\n",
    "    joint_anomaly_years = country_data_full.index[\n",
    "        country_data_full[\"iso_anomaly\"] & country_data_full[\"lof_anomaly\"]\n",
    "    ]\n",
    "\n",
    "    for year in iso_anomaly_years:\n",
    "        ax1.axvline(\n",
    "            x=year,\n",
    "            color=\"cyan\",\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.8,\n",
    "            label=\"IF only\" if year == iso_anomaly_years[0] else \"\",\n",
    "        )\n",
    "    for year in lof_anomaly_years:\n",
    "        ax1.axvline(\n",
    "            x=year,\n",
    "            color=\"magenta\",\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.8,\n",
    "            label=\"LOF only\" if year == lof_anomaly_years[0] else \"\",\n",
    "        )\n",
    "    for year in joint_anomaly_years:\n",
    "        ax1.axvline(\n",
    "            x=year,\n",
    "            color=\"red\",\n",
    "            linestyle=\"-\",\n",
    "            linewidth=2,\n",
    "            alpha=0.8,\n",
    "            label=\"Overlap\" if year == joint_anomaly_years[0] else \"\",\n",
    "        )\n",
    "\n",
    "    ax1.set_ylabel(\"Weighted Arms Transfers (TIV)\")\n",
    "    ax1.set_title(f\"{country_name} Arms Transfers: Volume\")\n",
    "    ax1.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Plot network connectivity (degrees)\n",
    "    ax2.plot(\n",
    "        country_data_full.index,\n",
    "        country_data_full[\"out_degree\"],\n",
    "        color=\"blue\",\n",
    "        linewidth=2,\n",
    "        label=\"Out Degree\",\n",
    "    )\n",
    "    ax2.plot(\n",
    "        country_data_full.index,\n",
    "        country_data_full[\"in_degree\"],\n",
    "        color=\"green\",\n",
    "        linewidth=2,\n",
    "        label=\"In Degree\",\n",
    "    )\n",
    "\n",
    "    for column_name, color in [(\"out_degree\", \"blue\"), (\"in_degree\", \"green\")]:\n",
    "        column_values = country_data_full[column_name]\n",
    "        valid_indices = np.where(~np.isnan(column_values.values))[0]\n",
    "        for i in range(len(valid_indices) - 1):\n",
    "            start_idx, end_idx = valid_indices[i], valid_indices[i + 1]\n",
    "            if end_idx - start_idx > 1:\n",
    "                ax2.plot(\n",
    "                    [column_values.index[start_idx], column_values.index[end_idx]],\n",
    "                    [column_values.iloc[start_idx], column_values.iloc[end_idx]],\n",
    "                    color=color,\n",
    "                    linestyle=\":\",\n",
    "                    linewidth=2,\n",
    "                    alpha=0.8,\n",
    "                )\n",
    "\n",
    "    # Highlight anomalies for connectivity\n",
    "    for year in iso_anomaly_years:\n",
    "        ax2.axvline(x=year, color=\"cyan\", linestyle=\"--\", alpha=0.8)\n",
    "    for year in lof_anomaly_years:\n",
    "        ax2.axvline(x=year, color=\"magenta\", linestyle=\"--\", alpha=0.8)\n",
    "    for year in joint_anomaly_years:\n",
    "        ax2.axvline(x=year, color=\"red\", linestyle=\"-\", linewidth=2, alpha=0.8)\n",
    "\n",
    "    ax2.set_ylabel(\"Number of Trade Partners (Degree)\")\n",
    "    ax2.set_xlabel(\"Year\")\n",
    "    ax2.set_title(f\"{country_name} Arms Transfers: Network Connectivity\")\n",
    "    ax2.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Add legend\n",
    "    fig.legend(loc=\"upper center\", ncol=4, frameon=False)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "    plt.savefig(f\"results/{country_name}_lineplot_anomalies.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pca_with_anomalies_combined(\n",
    "    country_name: str,\n",
    "    feature_columns: list = [\n",
    "        \"year_scaled\",\n",
    "        \"in_degree\",\n",
    "        \"out_degree\",\n",
    "        \"weighted_in_degree\",\n",
    "        \"weighted_out_degree\",\n",
    "    ],\n",
    "    plot_title: str = \"PCA Scatter Plot with Anomalies\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply PCA to reduce the dataset to 2 dimensions and plot a single scatter plot\n",
    "    with both Isolation Forest and LOF anomalies.\n",
    "\n",
    "    :param country_name: Name of the country to plot.\n",
    "    :param feature_columns: List of feature columns to use for PCA.\n",
    "    :param plot_title: Title of the plot.\n",
    "    :return: Saves the plot to disk.\n",
    "    \"\"\"\n",
    "\n",
    "    country_data = (\n",
    "        feature_engineered_arms_trade_df[\n",
    "            feature_engineered_arms_trade_df[\"country\"] == country_name\n",
    "        ]\n",
    "        .sort_values(\"year\")\n",
    "        .set_index(\"year\")\n",
    "    )\n",
    "\n",
    "    # Extract features and anomalies\n",
    "    features = country_data[feature_columns].dropna()\n",
    "    iso_anomaly_points = country_data.loc[features.index, \"iso_anomaly\"]\n",
    "    lof_anomaly_points = country_data.loc[features.index, \"lof_anomaly\"]\n",
    "\n",
    "    overlap_anomaly_points = iso_anomaly_points & lof_anomaly_points\n",
    "    iso_only_anomaly_points = iso_anomaly_points & ~lof_anomaly_points\n",
    "    lof_only_anomaly_points = lof_anomaly_points & ~iso_anomaly_points\n",
    "    normal_data_points = ~(iso_anomaly_points | lof_anomaly_points)\n",
    "\n",
    "    pca = PCA(n_components=5)\n",
    "    pca_transformed_features = pca.fit_transform(features)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(\n",
    "        pca_transformed_features[normal_data_points, 0],\n",
    "        pca_transformed_features[normal_data_points, 1],\n",
    "        c=\"blue\",\n",
    "        label=\"Normal\",\n",
    "        alpha=0.6,\n",
    "    )\n",
    "    plt.scatter(\n",
    "        pca_transformed_features[iso_only_anomaly_points, 0],\n",
    "        pca_transformed_features[iso_only_anomaly_points, 1],\n",
    "        c=\"cyan\",\n",
    "        label=\"IF only\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    plt.scatter(\n",
    "        pca_transformed_features[lof_only_anomaly_points, 0],\n",
    "        pca_transformed_features[lof_only_anomaly_points, 1],\n",
    "        c=\"magenta\",\n",
    "        label=\"LOF only\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    plt.scatter(\n",
    "        pca_transformed_features[overlap_anomaly_points, 0],\n",
    "        pca_transformed_features[overlap_anomaly_points, 1],\n",
    "        c=\"red\",\n",
    "        label=\"Overlap\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    plt.title(plot_title, fontsize=16)\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"results/{country_name}_scatterplot_anomalies.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_datapoints_per_year(df: pd.DataFrame, year_left: int, year_right: int):\n",
    "    \"\"\"\n",
    "    Plot number of data points per year with vertical lines indicating feasible bandwidth.\n",
    "    :param df: DataFrame with years as index and counts as values.\n",
    "    :param year_left: Minimum year of feasible bandwidth.\n",
    "    :param year_right: Maximum year of feasible bandwidth.\n",
    "    :return: Plot saved to disk.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df.index, df.values, marker=\"o\", linestyle=\"-\", color=\"blue\")\n",
    "    plt.title(\"Selected Feasible Bandwidth Based on Data Stability\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Number of Data Points\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "    plt.axvline(x=year_left, color=\"red\", linestyle=\":\", linewidth=2, alpha=0.8)\n",
    "    plt.axvline(x=year_right, color=\"red\", linestyle=\":\", linewidth=2, alpha=0.8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/data_points_per_year.png\", format=\"png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_evaluation_boxplot(evaluation_df, output_file='evaluation_boxplot.png'):\n",
    "    \"\"\"\n",
    "    Plots a Seaborn boxplot for the evaluation DataFrame and saves it as a local file.\n",
    "\n",
    "    :param evaluation_df: DataFrame containing evaluation metrics (e.g., iso_stability, lof_stability, model_agreement).\n",
    "    :param output_file: File name to save the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=evaluation_df, orient='h', palette='Set2')\n",
    "    plt.title('Evaluation Metrics Boxplot', fontsize=16)\n",
    "    plt.xlabel('Values', fontsize=12)\n",
    "    plt.ylabel('Metrics', fontsize=12)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, format='png', dpi=300)\n",
    "    plt.close()"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "6b654c333704550a",
   "metadata": {},
   "source": [
    "# =========================\n",
    "# Load SIPRI Data and Preprocess\n",
    "# =========================\n",
    "\n",
    "# CSV must have columns: year, exporter, importer, tiv\n",
    "arms_trade_dataframe = pd.read_csv(\"data/trade-register.csv\")\n",
    "\n",
    "arms_trade_dataframe.info()\n",
    "\n",
    "# Selecting relevant columns\n",
    "\n",
    "arms_trade_dataframe = arms_trade_dataframe[\n",
    "    [\"Year of order\", \"Supplier\", \"Recipient\", \"SIPRI TIV for total order\"]\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "578e0e8112abdf4f",
   "metadata": {},
   "source": [
    "# Removing rows with missing values\n",
    "print(arms_trade_dataframe.isna().sum())\n",
    "arms_trade_dataframe = arms_trade_dataframe.dropna()\n",
    "print(arms_trade_dataframe.isna().sum())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80233bbf3e9ae492",
   "metadata": {},
   "source": [
    "# Check for negative TIV values and remove them\n",
    "arms_trade_dataframe = arms_trade_dataframe[\n",
    "    arms_trade_dataframe[\"SIPRI TIV for total order\"] >= 0\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a5a7d357d4d39449",
   "metadata": {},
   "source": [
    "# ========================================================================\n",
    "#  Determine Feasible Bandwidth Based on Data Precision and trim dataframe\n",
    "# ========================================================================\n",
    "\n",
    "counts = (\n",
    "    arms_trade_dataframe.groupby(\"Year of order\")\n",
    "    .size()\n",
    "    .reset_index(name=\"n_obs\")\n",
    "    .sort_values(\"Year of order\")\n",
    ")\n",
    "counts[\"se_proxy\"] = 1 / np.sqrt(counts[\"n_obs\"])\n",
    "counts[\"relative_se\"] = counts[\"se_proxy\"] / counts[\"se_proxy\"].min()\n",
    "\n",
    "precision_threshold = 2.0\n",
    "\n",
    "counts[\"precise_enough\"] = counts[\"relative_se\"] <= precision_threshold\n",
    "\n",
    "precision_year_left = counts.loc[counts[\"precise_enough\"], \"Year of order\"].min()\n",
    "precision_year_right = counts.loc[counts[\"precise_enough\"], \"Year of order\"].max()\n",
    "\n",
    "data_points_per_year = arms_trade_dataframe.groupby(\"Year of order\").size()\n",
    "plot_datapoints_per_year(\n",
    "    data_points_per_year, precision_year_left, precision_year_right\n",
    ")\n",
    "\n",
    "# reset dataframe to feasible bandwidth\n",
    "arms_trade_dataframe_trimmed = arms_trade_dataframe[\n",
    "    (arms_trade_dataframe[\"Year of order\"] >= precision_year_left)\n",
    "    & (arms_trade_dataframe[\"Year of order\"] <= precision_year_right)\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ad47d8de194fb310",
   "metadata": {},
   "source": [
    "# =========================\n",
    "#  Create Yearly Graphs\n",
    "# =========================\n",
    "graphs_by_year = {}\n",
    "for year, year_df in arms_trade_dataframe_trimmed.groupby(\"Year of order\"):\n",
    "    G = nx.DiGraph(year=year)\n",
    "    for _, row in year_df.iterrows():\n",
    "        exporter = row[\"Supplier\"]\n",
    "        importer = row[\"Recipient\"]\n",
    "        tiv = row[\"SIPRI TIV for total order\"]\n",
    "        if G.has_edge(exporter, importer):\n",
    "            G[exporter][importer][\"weight\"] += tiv\n",
    "        else:\n",
    "            G.add_edge(exporter, importer, weight=tiv)\n",
    "    graphs_by_year[year] = G\n",
    "\n",
    "# =========================\n",
    "# Compute Node Features\n",
    "# =========================\n",
    "all_features = []\n",
    "for year, G in graphs_by_year.items():\n",
    "    for node in G.nodes:\n",
    "        all_features.append(\n",
    "            {\n",
    "                \"year\": year,\n",
    "                \"country\": node,\n",
    "                \"in_degree\": G.in_degree(node),\n",
    "                \"out_degree\": G.out_degree(node),\n",
    "                \"weighted_in_degree\": G.in_degree(node, weight=\"weight\"),\n",
    "                \"weighted_out_degree\": G.out_degree(node, weight=\"weight\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "feature_engineered_arms_trade_df = pd.DataFrame(all_features)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "288e3b02809af210",
   "metadata": {},
   "source": [
    "# ===============================\n",
    "# Build full countryâ€“year grid\n",
    "# ===============================\n",
    "\n",
    "feature_engineered_arms_trade_df = (\n",
    "    feature_engineered_arms_trade_df.set_index([\"year\", \"country\"])\n",
    "    .reindex(\n",
    "        pd.MultiIndex.from_product(\n",
    "            [\n",
    "                feature_engineered_arms_trade_df[\"year\"].unique(),\n",
    "                feature_engineered_arms_trade_df[\"country\"].unique(),\n",
    "            ],\n",
    "            names=[\"year\", \"country\"],\n",
    "        )\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# ================================================================\n",
    "# Rescale years linearly for extra feature in anomaly detection\n",
    "# ================================================================\n",
    "\n",
    "feature_engineered_arms_trade_df[\"year_scaled\"] = MinMaxScaler().fit_transform(\n",
    "    feature_engineered_arms_trade_df[[\"year\"]]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56bef6f0d3ad1725",
   "metadata": {},
   "source": [
    "feature_cols = [\n",
    "    \"year_scaled\",\n",
    "    \"in_degree\",\n",
    "    \"out_degree\",\n",
    "    \"weighted_in_degree\",\n",
    "    \"weighted_out_degree\",\n",
    "]\n",
    "\n",
    "\n",
    "# Determine minimum data points required per country based on feasible bandwidth, 80% selected\n",
    "minimum_data_points = np.round((precision_year_right - precision_year_left) * 0.8, 0)\n",
    "print(f\"Minimum data points per country: {minimum_data_points}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c74091a619d4be33",
   "metadata": {},
   "source": [
    "# ===============================================================================================================================\n",
    "# Train and fit IsolationForest, LocalOutlierFactor, and combine them per country. Evaluate Country Performace and store in DF\n",
    "# ==============================================================================================================================\n",
    "\n",
    "evaluation_rows = []\n",
    "\n",
    "feature_engineered_arms_trade_df[\"iso_anomaly\"] = False\n",
    "feature_engineered_arms_trade_df[\"lof_anomaly\"] = False\n",
    "feature_engineered_arms_trade_df[\"combined_anomaly\"] = False\n",
    "\n",
    "for country, df_c in tqdm.tqdm(\n",
    "    feature_engineered_arms_trade_df.groupby(\"country\"), desc=\"Processing countries\"\n",
    "):\n",
    "    df_c = df_c.sort_values(\"year_scaled\").dropna()\n",
    "\n",
    "    if len(df_c) < minimum_data_points:\n",
    "        continue\n",
    "\n",
    "    X = df_c[feature_cols].values\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    best_param_iso = tune_isolation_forest(X)\n",
    "    best_k = tune_lof(X)\n",
    "\n",
    "    # Fit Isolation Forest\n",
    "    iso = IsolationForest(\n",
    "        n_estimators=best_param_iso[0], contamination=best_param_iso[1], random_state=42\n",
    "    )\n",
    "    iso_scores = iso.fit(X).decision_function(X)\n",
    "    iso_labels = iso_scores < np.percentile(iso_scores, 100 * best_param_iso[1])\n",
    "\n",
    "    # Fit LOF\n",
    "    lof = LocalOutlierFactor(n_neighbors=best_k)\n",
    "    lof_labels = lof.fit_predict(X) == -1\n",
    "    lof_scores = -lof.negative_outlier_factor_\n",
    "\n",
    "    # Combine LOF and IF\n",
    "    combined_scores = (iso_scores + lof_scores) / 2  # Average of scores\n",
    "    combined_labels = combined_scores < np.percentile(\n",
    "        combined_scores, 100 * best_param_iso[1]\n",
    "    )\n",
    "\n",
    "    # Update features_df with anomaly labels\n",
    "    feature_engineered_arms_trade_df.loc[df_c.index, \"iso_anomaly\"] = iso_labels\n",
    "    feature_engineered_arms_trade_df.loc[df_c.index, \"lof_anomaly\"] = lof_labels\n",
    "    feature_engineered_arms_trade_df.loc[df_c.index, \"combined_anomaly\"] = (\n",
    "        combined_labels\n",
    "    )\n",
    "\n",
    "    # Number of runs for stability calculation\n",
    "    n_runs = 50\n",
    "    \n",
    "    # Generate multiple runs of Isolation Forest scores\n",
    "    iso_scores_list = []\n",
    "    for i in range(n_runs):\n",
    "        iso = IsolationForest(\n",
    "            n_estimators=best_param_iso[0],\n",
    "            contamination=best_param_iso[1],\n",
    "            random_state=i  # Use different random states for each run\n",
    "        )\n",
    "        iso_scores_list.append(iso.fit(X).decision_function(X))\n",
    "    \n",
    "    # Calculate Isolation Forest stability\n",
    "    iso_stability = calculate_model_stability(iso_scores_list)\n",
    "    \n",
    "    # Generate multiple runs of LOF scores\n",
    "    lof_scores_list = []\n",
    "    for i in range(n_runs):\n",
    "        lof = LocalOutlierFactor(n_neighbors=best_k)\n",
    "        lof.fit(X)\n",
    "        lof_scores_list.append(-lof.negative_outlier_factor_)\n",
    "    \n",
    "    # Calculate LOF stability\n",
    "    lof_stability = calculate_model_stability(lof_scores_list)\n",
    "\n",
    "    # Model Agreement\n",
    "    model_agreement = calculate_model_agreement(iso_labels, lof_labels)\n",
    "\n",
    "    # Append evaluation results\n",
    "    evaluation_rows.append(\n",
    "        {\n",
    "            \"country\": country,\n",
    "            \"iso_stability\": iso_stability,\n",
    "            \"lof_stability\": lof_stability,\n",
    "            \"model_agreement\": model_agreement,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Convert evaluation results to DataFrame\n",
    "evaluation_df = pd.DataFrame(evaluation_rows)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d053de74bc1d480",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T18:38:43.483035Z",
     "start_time": "2026-01-27T18:38:43.307205Z"
    }
   },
   "source": [
    "evaluation_df.describe()\n",
    "plot_evaluation_boxplot(evaluation_df, output_file='results/evaluation_boxplot.png')"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "a9bed7a0bba06239",
   "metadata": {},
   "source": [
    "countries = [\"Netherlands\", \"Poland\", \"Ukraine\"]\n",
    "\n",
    "for country in countries:\n",
    "    plot_country_anomalies(country)\n",
    "    plot_pca_with_anomalies_combined(country)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b1d5275d014b5dfd",
   "metadata": {},
   "source": [
    "# Based On https://github.com/ngoix/EMMV_benchmarks/blob/master/em.py\n",
    "\n",
    "# Parameters of the algorithm and data\n",
    "\n",
    "n_generated = 100000  # Number of generated uniform samples\n",
    "alpha_min = 0.9  # Minimum alpha value for MV curve\n",
    "alpha_max = 0.999  # Maximum alpha value for MV curve\n",
    "t_max = 0.9  # Maximum t threshold for EM curve\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize figure for plotting\n",
    "plt.figure(figsize=(25, 13))\n",
    "\n",
    "# Load and vectorize the feature data\n",
    "X = feature_engineered_arms_trade_df[feature_cols].dropna().values\n",
    "\n",
    "# Split data into training and testing sets\n",
    "n_samples, n_features = np.shape(X)\n",
    "n_samples_train = n_samples // 2\n",
    "n_samples_test = n_samples - n_samples_train\n",
    "\n",
    "X_train = X[:n_samples_train, :]  # Training data\n",
    "X_test = X[n_samples_train:, :]  # Testing data\n",
    "\n",
    "# Tune hyperparameters for Isolation Forest and LOF\n",
    "best_param_iso = tune_isolation_forest(X)\n",
    "best_k = tune_lof(X)\n",
    "\n",
    "# ---------------------------\n",
    "# Fit Isolation Forest\n",
    "# ---------------------------\n",
    "iforest = IsolationForest(\n",
    "    n_estimators=best_param_iso[0],  # Best number of estimators\n",
    "    contamination=best_param_iso[1],  # Best contamination level\n",
    ")\n",
    "\n",
    "# Fit Local Outlier Factor with the best number of neighbors\n",
    "lof = LocalOutlierFactor(n_neighbors=best_k, novelty=True)\n",
    "\n",
    "# Calculate the limits of the feature space\n",
    "lim_inf = X.min(axis=0)  # Minimum values for each feature\n",
    "lim_sup = X.max(axis=0)  # Maximum values for each feature\n",
    "\n",
    "# Calculate the volume of the support (bounding box of the feature space)\n",
    "volume_support = (lim_sup - lim_inf).prod()\n",
    "\n",
    "# Check if the volume is valid\n",
    "if volume_support <= 0:\n",
    "    raise ValueError(\"volume_support is zero or negative. Check your input data.\")\n",
    "\n",
    "# Generate t values and alpha values for EM and MV curves\n",
    "step_size = max(0.01 / volume_support, 1e-10)  # Ensure step size is not too small\n",
    "t = np.arange(0, 100 / volume_support, step_size)  # t values for EM curve\n",
    "axis_alpha = np.arange(alpha_min, alpha_max, 0.0001)  # Alpha values for MV curve\n",
    "\n",
    "# Generate uniform samples within the feature space\n",
    "unif = np.random.uniform(lim_inf, lim_sup, size=(n_generated, n_features))\n",
    "\n",
    "# Fit Isolation Forest and calculate decision scores\n",
    "print(\"IsolationForest processing...\")\n",
    "iforest.fit(X_train)\n",
    "s_X_iforest = iforest.decision_function(X_test)  # Scores for test data\n",
    "s_unif_iforest = iforest.decision_function(unif)  # Scores for uniform samples\n",
    "\n",
    "# Fit Local Outlier Factor and calculate decision scores\n",
    "print(\"LocalOutlierFactor processing...\")\n",
    "lof.fit(X_train)\n",
    "s_X_lof = lof.decision_function(X_test)  # Scores for test data\n",
    "s_unif_lof = lof.decision_function(unif)  # Scores for uniform samples\n",
    "\n",
    "# Plot Excess-Mass (EM) curve\n",
    "plt.subplot(121)\n",
    "auc_iforest, em_iforest, amax_iforest = em(\n",
    "    t, t_max, volume_support, s_unif_iforest, s_X_iforest, n_generated\n",
    ")\n",
    "auc_lof, em_lof, amax_lof = em(\n",
    "    t, t_max, volume_support, s_unif_lof, s_X_lof, n_generated\n",
    ")\n",
    "\n",
    "# Determine the maximum t index for plotting\n",
    "if amax_iforest == -1 or amax_lof == -1:\n",
    "    amax = -1\n",
    "else:\n",
    "    amax = max(amax_iforest, amax_lof)\n",
    "\n",
    "# Plot EM curves for Isolation Forest and LOF\n",
    "plt.plot(\n",
    "    t[:amax],\n",
    "    em_iforest[:amax],\n",
    "    lw=1,\n",
    "    label=\"%s (em_score = %0.3e)\" % (\"iforest\", auc_iforest),\n",
    ")\n",
    "plt.plot(\n",
    "    t[:amax], em_lof[:amax], lw=1, label=\"%s (em-score = %0.3e)\" % (\"lof\", auc_lof)\n",
    ")\n",
    "\n",
    "# Customize EM plot\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel(\"t\", fontsize=20)\n",
    "plt.ylabel(\"EM(t)\", fontsize=20)\n",
    "plt.title(\"Excess-Mass curve for dataset\", fontsize=20)\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Plot Mass-Volume (MV) curve\n",
    "plt.subplot(122)\n",
    "auc_iforest, mv_iforest = mv(\n",
    "    axis_alpha, volume_support, s_unif_iforest, s_X_iforest, n_generated\n",
    ")\n",
    "auc_lof, mv_lof = mv(axis_alpha, volume_support, s_unif_lof, s_X_lof, n_generated)\n",
    "\n",
    "# Plot MV curves for Isolation Forest and LOF\n",
    "plt.plot(\n",
    "    axis_alpha,\n",
    "    mv_iforest,\n",
    "    lw=1,\n",
    "    label=\"%s (mv-score = %0.3e)\" % (\"iforest\", auc_iforest),\n",
    ")\n",
    "plt.plot(axis_alpha, mv_lof, lw=1, label=\"%s (mv-score = %0.3e)\" % (\"lof\", auc_lof))\n",
    "\n",
    "# Customize MV plot\n",
    "plt.xlabel(\"alpha\", fontsize=20)\n",
    "plt.ylabel(\"MV(alpha)\", fontsize=20)\n",
    "plt.title(\"Mass-Volume Curve for dataset\", fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
